<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="style.css" type="text/css">
    <link rel="stylesheet" href="print.css" media="print" type="text/css">

    <title>The Structure and Interpretation of Deep Networks</title>
  </head>

  <body>

<header id="top" style="background: black;
            background-size: cover;
            background-repeat: repeat-x;
            ">
      <div class="inner">
      <h1 style="font: bold 50pt Helvetica, Arial, sans-serif; color: white;
        line-height: 0.8; letter-spacing: -3px; margin-bottom: 20px;">
        Structure and <div>Interpretation of</div>
        Deep Networks 
      </h1>
      <h2>A Deep Learning Practicum</h2>
      <h2>IAP 2020</h2>
      <div class='mymenu'>
         <span class='menu-link'><a href='/#main-content'>Home</a></span>
         <span class='menu-link-last'><a href='https://forms.gle/b23TECfspLwEpFUTA'>Sign-up form</a></span>
      </div>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h1>Structure and Interpretation of <nobr>Deep Networks</nobr></h2>
          <p>Deep networks program themselves by finding patterns in data.
          But what patterns do they find,
          and what programs do they learn?
          Large-scale machine learning presents computer science researchers
          and practitioners with a new problem: how to understand, apply,
          and improve complex programs that were not designed explicitly
          by a human programmer.
          </p>
          <p>In this IAP practicum, we introduce the problem of
          interpreting deep networks.  We will not focus on training.
          Rather, we our topic is the art of how to understand a network
          after it is trained.  We examine both local
          (single-case) and global (over-a-distribution) methods for
          understanding a network's behavior.

          <p>Every class will be a practicum, built around an interactive
          lab where you will learn to apply explanation methods using
          real models and data.  Interpretable machine learning is a quickly
          evolving area, and each class will be built around recent papers,
          and will be led by researchers who are active in the area.

          <p><a href="https://forms.gle/n1TdXPNu5oomefUEA" style="text-decoration:line-through">Enroll here (limited to 30)</a></p>
          <p>Location: <b>E17-134</b></p>

          <h1>Prerequisites</h1>

          <p>The course assumes familiarity with basic machine learning,
          neural networks, and python.  An introductory machine learning
          course or a deep network course such as S.191 should be enough.
          You should have previous experience training models; this class
          is not mainly about learning to train models, but about
          understanding trained models.
          </p>


          <h1>Syllabus</h1>
          <h2>1. Introduction: Why care how?</h2>
          <p>Does it matter whether we understand what a
          deep network is doing or not, if the results look good?
          Isn't accuracy on the holdout set the ultimate goal?
          <p>Reading:
          <a href="https://arxiv.org/pdf/1702.08608.pdf">Doshi-Velez, Kim,
          Towards a Rigorous Science</a>;
          <a href="https://arxiv.org/pdf/1606.03490.pdf">Lipton, Mythos
          of Model Interpretability</a>;
          <a href="https://arxiv.org/pdf/1703.03717.pdf">Ross,
          Right for the Right Reasons</a>;
          <a href="https://arxiv.org/pdf/1812.08951.pdf">Belinkov, Analysis methods
          in NLP</a>
          <p>Andrew Ross (Harvard), Yonatan Belinkov (Harvard+CSAIL), Julius Adebayo (CSAIL), David Bau (CSAIL).
          <br>Practicum: <a href="http://bit.ly/sidn-colab-1">Decoy MNIST</a>
          <br>Materials: <a href="slides/intro.pptx">Slides</a>
          <a href="https://github.com/SIDN-IAP/intro-lab">github</a>
          <br>2:30-4:00pm, Friday January 17, 2020.

          <h2>2. Explaining predictions.</h2>
          <p>What part of the input does a model look at?  What can
          we learn from this?
          <p>Reading:
          <a href="https://arxiv.org/abs/1706.03825"
             >Smilkov, SmoothGrad</a>;
          <a href="https://arxiv.org/pdf/1806.07421.pdf"
             >Petsiuk, RISE</a>;
          <a href="https://distill.pub/2020/attribution-baselines/"
             >Sturmfels, Feature attribution</a>;
          <a href="https://arxiv.org/abs/1906.10282"
             >Ding, Saliency-driven Word Alignment Interpretation</a>
          <p>Sebastian Gehrmann (Harvard), Mirac Suzgun (Harvard), Vitali Petsiuk (BU), Julius Adebayo (CSAIL).
          <br>Practicum: Saliency methods in vision and NLP.
          <br>2:30-4:00pm, Tuesday January 21, 2020.

          <h2>3. Explaining models.</h2>
          <p>How does a neural network decompose data internally?  What can
          we learn from a model's representation?
          <p>Reading:
          <a href="https://arxiv.org/abs/1812.08951">Belinkov and Glass,
            Analysis Methods (section 2)</a>;
          <a href="https://arxiv.org/pdf/1704.05796.pdf"
             >Bau, Network Dissection</a>;
          <a href="https://arxiv.org/pdf/1811.10597.pdf"
             >GAN Dissection</a>
          <br>Optional:
          <a href="https://arxiv.org/abs/1608.04207">Adi,
            Fine-grained Analysis of Sentence Embeddings</a>;
          <a href="https://jair.org/index.php/jair/article/view/11196/26408"
             >Hupkes, Visualization and diagnostic classifiers</a>;
          <a href="https://www.aclweb.org/anthology/D19-1275.pdf"
             >Hewitt and Liang, Designing and Interpreting Probes</a>;
          <a href="https://distill.pub/2017/feature-visualization/"
             >Olah, Lucid Feature Visualization</a>;
          <a href="https://arxiv.org/pdf/1711.11279.pdf"
             >Kim, Concept Activation Vectors</a>
          <p>David Bau (CSAIL), Yonatan Belinkov (Harvard+CSAIL)
          <br>Practicum: NeuroX, Network dissection
          <br>2:30-4:00pm, Wednesday January 22, 2020.
          
          <h2>4. Adversaries and interpretability.</h2>
          <p>Why do some interpretability methods fail to uncover
          reasonable explations? Could this be a consequence of how
          models actually make decisions? We will discuss recent
          findings suggesting that ML models rely on features that
          are imperceptible to humans. Then we will see how training
          models to be robust to imperceptible input changes can
          lead to models that rely on more human-aligned features.
          </p>
          <p>
          Shibani Santurkar (CSAIL), Dimitris Tsipras (CSAIL).
          <br>
          Practicum: Explore simple gradient explanations
          for standard and robust models
          <br>Readings:
          <a href="https://arxiv.org/abs/1412.6572"
          >Generating adversarial examples with FGSM</a>;
          <a href="https://arxiv.org/abs/1706.03825"
             >Simple gradient explanation with SmoothGrad</a>
          <br>Optional:
          <a href="https://arxiv.org/abs/1706.06083"
             >Training robust models with robust optimization</a>;
          <a href="https://arxiv.org/abs/1905.02175"
             >ML models rely on imperceptible features</a>;
          <a href="https://arxiv.org/abs/1805.12152"
             >Robustness vs Accuracy</a>;
          <a href="https://arxiv.org/abs/1906.00945"
             >Robustness as a feature prior</a>
          <br>2:30-4:00pm, Friday January 24, 2020.
          
          <h2>5. Bias and fairness.</h2>
          <p>Big data and deep learning may unintentionally amplify bias
            present in the dataset collection or model formulation process. How 
            can we define "fairness" in a quantitative way? How can we audit a 
            system for bias and potential for disparate impact? How can we create
            equitable models?
          <p>Irene Chen (CSAIL)
          <br>Reading:
            <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing"
              >Machine Bias (COMPAS data to be used in the lab)</a>;
            <a href="http://proceedings.mlr.press/v81/buolamwini18a/buolamwini18a.pdf">
              Gender Shades</a>;
            <a href="http://www.andrew.cmu.edu/user/achoulde/files/disparate_impact.pdf">
              Fair prediction with disparate impact</a>;
            <a href="https://science.sciencemag.org/content/366/6464/447">
              Dissecting racial bias in an algorithm used to manage the health of populations</a>
          <br>Practicum: COMPAS
          <br>2:30-4:00pm, Monday January 27, 2020.

          <h2>6. Interaction and collaboration.</h2>
          <p>How can interactive methods help us to formulate hypotheses
          about models and data? What can we learn about the structure of
          a model by posing counterfactual "what if" questions? How can
          humans collaborate with machine-learned models? We will describe
          some common methods to create interactive AI tools and will create
          two very small examples for interactions with a text and an
          image model.
          <p>Hendrik Strobelt (IBM), David Bau (CSAIL)
          <br>Reading:
          <a href="https://arxiv.org/abs/1804.09299">Strobelt, Seq2Seq-Vis</a>;
          <a href="http://ganpaint.io/">Bau, GANPaint</a>;
          <a href="https://arxiv.org/abs/1907.10739"
            >Gehrmann, Collaborative Semantic Inference</a>.
          <br>Practicum: Seq2Seq-Vis, GANPaint, CSI.
          <br>2:30-4:00pm, Wednesday January 29, 2020.

          <h2>7. Complex explanations.</h2>
          <p>Sometimes representations and decisions in models are
          hard to communicate with examples and visualizations.
          How can we use richer explanations (especially in language)
          to describe more complex behaviors?</p>
          <p>Jacob Andreas (CSAIL), Jesse Mu (Stanford)
          <br>Practicum: Generating natural language explanations.
          <br>2:30-4:00pm, Thursday January 30, 2020.
          <b style="color:red">Note new date</b>
        </section>

      </div>
      <div class="inner clearfix" id="sponsorships">
        <h1>Sponsored by</h1>
        <div style="text-align-last: justify; text-align: justify">
          <img src="logo_quest_black_v3.png">
          <img src="logo_IBMwatson_v1.jpg" style="margin: 0 25px;">
        </div>
      </div>
    </div>

<footer>
&nbsp;
</footer>
    
  </body>
</html>

