<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="style.css" type="text/css">
    <link rel="stylesheet" href="print.css" media="print" type="text/css">

    <title>The Structure and Interpretation of Deep Networks</title>
  </head>

  <body>

<header id="top" style="background: black;
            background-size: cover;
            background-repeat: repeat-x;
            ">
      <div class="inner">
      <h1 style="font: bold 50pt Helvetica, Arial, sans-serif; color: white;
        line-height: 0.8; letter-spacing: -3px; margin-bottom: 20px;">
        Structure and <div>Interpretation of</div>
        Deep Networks 
      </h1>
      <h2>A Deep Learning Practicum</h2>
      <h2>IAP 2020</h2>
      <div class='mymenu'>
         <span class='menu-link'><a href='/#main-content'>Home</a></span>
         <span class='menu-link-last'><a href='https://forms.gle/n1TdXPNu5oomefUEA'>Sign-up form</a></span>
      </div>
      </div>
    </header>

    <div id="content-wrapper">
      <div class="inner clearfix">
        <section id="main-content">
          <h1>Structure and Interpretation of <nobr>Deep Networks</nobr></h2>
          <p>Deep networks program themselves by finding patterns in data.
          But what patterns do they find,
          and what programs do they learn?
          Large-scale machine learning presents computer science researchers
          and practitioners with a new problem: how to understand, apply,
          and improve complex programs that were not designed explicitly
          by a human programmer.
          </p>
          <p>In this IAP practicum, we introduce the problem of
          interpreting deep networks.  We will not focus on training.
          Rather, we our topic is the art of how to understand a network
          after it is trained.  We examine both local
          (single-case) and global (over-a-distribution) methods for
          understanding a network's behavior.

          <p>Every class will be a practicum, built around an interactive
          lab where you will learn to apply explanation methods using
          real models and data.  Interpretable machine learning is a quickly
          evolving area, and each class will be built around recent papers,
          and will be led by researchers who are active in the area.

          <p><a href="https://forms.gle/n1TdXPNu5oomefUEA">Enroll here (limited to 30)</a></p>
          <p>Location: <b>E17-134</b></p>

          <h1>Prerequisites</h1>

          <p>The course assumes familiarity with basic machine learning,
          neural networks, and python.  An introductory machine learning
          course or a deep network course such as S.191 should be enough.
          You should have previous experience training models; this class
          is not mainly about learning to train models, but about
          understanding trained models.
          </p>


          <h1>Syllabus</h1>
          <h2>1. Introduction: Why care how?</h2>
          <p>Does it matter whether we understand what a
          deep network is doing or not, if the results look good?
          Isn't accuracy on the holdout set the ultimate goal?
          <p>Reading:
          <a href="https://arxiv.org/pdf/1702.08608.pdf">Doshi-Velez, Kim,
          Towards a Rigorous Science</a>;
          <a href="https://arxiv.org/pdf/1606.03490.pdf">Lipton, Mythos
          of Model Interpretability</a>;
          <a href="https://arxiv.org/pdf/1703.03717.pdf">Ross,
          Right for the Right Reasons</a>
          <p>Andrew Ross (Harvard), Yonatan Belinkov (Harvard+CSAIL), Julius Adebayo (CSAIL), David Bau (CSAIL).
          <br>Practicum: Decoy MNIST.
          <br>2:30-4:00pm, Friday January 17, 2020.

          <h2>2. Explaining predictions.</h2>
          <p>What part of the input does a model look at?  What can
          we learn from this?
          <!--
          <p>Reading:
          <a href="">Grad-CAM</a>
          <a href="">RISE</a>
          <a href="">LRP</a>
          <a href="">SmoothGrad</a>
          <a href="">Benchmark</a>
          -->
          <p>Sebastian Gehrmann (Harvard), Mirac Suzgun (Harvard), Vitali Petsiuk (BU), Julius Adebayo (CSAIL).
          <br>Practicum: Saliency methods in vision and NLP.
          <br>2:30-4:00pm, Tuesday January 21, 2020.

          <h2>3. Explaining models.</h2>
          <p>How does a neural network decompose data internally?  What can
          we learn from a model's representation?
          <!--
          <p>Reading:
          <a href="">Network dissection</a>
          <a href="">Lucid visualization</a>
          (Adi et al., 2017b, Veldhoen et al., 2016, Conneau et al., 2018)
          -->
          <p>David Bau (CSAIL), Yonatan Belinkov (Harvard+CSAIL)
          <br>Practicum: NeuroX, Network dissection
          <br>2:30-4:00pm, Wednesday January 22, 2020.
          
          <h2>4. Adversaries and interpretability.</h2>
          <p>Recent findings suggest that adversarial robustness may
          lead to more interpretable models.  We discuss and experiment with
          this effect.
          <!--
          <a href="">Madry papers</a>
          -->
          <p>Shibani Santurkar (CSAIL), Dimitris Tsipras (CSAIL). 
          <br>Practicum: Generative model from a robust classifier
          <br>2:30-4:00pm, Friday January 24, 2020.
          
          <h2>5. Bias and fairness.</h2>
          <p>Irene Chen (CSAIL)
          <br>Practicum: TBD
          <br>2:30-4:00pm, Monday January 27, 2020.

          <h2>6. Interaction and collaboration.</h2>
          <p>How can interactive methods help us to formulate hypotheses
          about models and data? What can we learn about the structure of
          a model by posing counterfactual "what if" questions? How can
          humans collaborate with machine-learned models? We will describe
          some common methods to create interactive AI tools and will create
          two very small examples for interactions with a text and an
          image model.
          <p>Hendrik Strobelt (IBM), David Bau (CSAIL)
          <br>Reading:
          <a href="https://arxiv.org/abs/1804.09299">Strobelt, Seq2Seq-Vis</a>;
          <a href="http://ganpaint.io/">Bau, GANPaint</a>;
          <a href="https://arxiv.org/abs/1907.10739"
            >Gehrmann, Collaborative Semantic Inference</a>.
          <br>Practicum: Seq2Seq-Vis, GANPaint, CSI.
          <br>2:30-4:00pm, Wednesday January 29, 2020.

          <h2>7. Making interpretable models.</h2>
          <p>What approaches can we use to train models that are
          interpretable by-design?
          <p>Jacob Andreas (CSAIL), Jesse Mu (Stanford)
          <br>Practicum: Modular neural networks
          <br>2:30-4:00pm, Thursday January 30, 2020.
          <b style="color:red">Note new date</b>
        </section>
      </div>
    </div>

<footer>
</footer>
    
  </body>
</html>

